#include "src/utf8_tokenizer.h"

#include <memory>
#include <string>
#include <thread>
#include <vector>

#include "gtest/gtest.h"

#include "src/cottontail.h"

TEST(Utf8Tokenizer, Split) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer =
      cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::vector<std::string> tokens;
  tokens = tokenizer->split("hello world");
  ASSERT_EQ(tokens.size(), 2);
  ASSERT_EQ(tokens[0], "hello");
  ASSERT_EQ(tokens[1], "world");
  tokens = tokenizer->split("HeLlO     wOrLd");
  ASSERT_EQ(tokens.size(), 2);
  ASSERT_EQ(tokens[0], "hello");
  ASSERT_EQ(tokens[1], "world");
  tokens = tokenizer->split("");
  ASSERT_EQ(tokens.size(), 0);
  tokens = tokenizer->split("ã€‚. ...");
  ASSERT_EQ(tokens.size(), 0);
  const char *libai[] = {
      "è‡ªé£ (æç™½)  Entertaining myself (Li Bai)",
      "",
      "å¯¹é…’ä¸è§‰ç‘   Too drunk to notice it had gotten dark,",
      "è½èŠ±ç›ˆæˆ‘è¡£ã€‚ Or that flowers had fallen all over my clothes,",
      "é†‰èµ·æ­¥æºªæœˆï¼Œ I stagger up and walk along the moonlit stream,",
      "é¸Ÿè¿˜äººäº¦ç¨€ã€‚ The birds have gone home; most of the people too."};
  std::string s;
  size_t n = sizeof(libai) / sizeof(char *);
  for (size_t i = 0; i < n; i++)
    s += libai[i] + std::string("\n");
  tokens = tokenizer->split(s);
  ASSERT_EQ(tokens.size(), 64);
  ASSERT_EQ(tokens[0], "è‡ª");
  ASSERT_EQ(tokens[2], "æ");
  ASSERT_EQ(tokens[6], "li");
  ASSERT_EQ(tokens[7], "bai");
  ASSERT_EQ(tokens[24], "æˆ‘");
  ASSERT_EQ(tokens[34], "clothes");
  ASSERT_EQ(tokens[40], "i");
  ASSERT_EQ(tokens[53], "ç¨€");
  ASSERT_EQ(tokens[54], "the");
  ASSERT_EQ(tokens[62], "people");
  ASSERT_EQ(tokens[63], "too");
  tokens = tokenizer->split(
      "ÎœÎ®Î½Ï…Î¼Î± Ï„Î·Ï‚ Î ÏÎ¿Î­Î´ÏÎ¿Ï… Ï„Î·Ï‚ Î”Î·Î¼Î¿ÎºÏÎ±Ï„Î¯Î±Ï‚ ÎšÎ±Ï„ÎµÏÎ¯Î½Î±Ï‚ Î£Î±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï… Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ "
      "Î±Ï€ÏŒÎ´Î·Î¼Î¿ Î•Î»Î»Î·Î½Î¹ÏƒÎ¼ÏŒ Î¼Îµ Ï„Î·Î½ ÎµÏ…ÎºÎ±Î¹ÏÎ¯Î± Ï„Î·Ï‚ ÎµÎ¸Î½Î¹ÎºÎ®Ï‚ ÎµÎ¿ÏÏ„Î®Ï‚ Ï„Î·Ï‚ 25Î·Ï‚ ÎœÎ±ÏÏ„Î¯Î¿Ï….");
  ASSERT_EQ(tokens.size(), 20);
  ASSERT_EQ(tokens[0], "Î¼Î®Î½Ï…Î¼Î±");
  ASSERT_EQ(tokens[6], "ÏƒÎ±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï…");
  ASSERT_EQ(tokens[18], "25Î·Ïƒ");
  ASSERT_EQ(tokens[19], "Î¼Î±ÏÏ„Î¯Î¿Ï…");
  tokens =
      tokenizer->split("ğŸ’™Blue HeartğŸ’—Growing Heart ğŸ’šç»¿è‰²çˆ±å¿ƒğŸ’– Sparkling "
                       "Heart ğŸ’“ Beating Heart ğŸ–¤ Black Heart ğŸ’Ÿ Heart "
                       "Decoration ğŸ’” Broken Heart ğŸ’› Yellow Heart");

  ASSERT_EQ(tokens.size(), 29);
  ASSERT_EQ(tokens[0], "ğŸ’™");
  ASSERT_EQ(tokens[5], "heart");
  ASSERT_EQ(tokens[10], "å¿ƒ");
  ASSERT_EQ(tokens[14], "ğŸ’“");
  ASSERT_EQ(tokens[28], "heart");
  tokens = tokenizer->split(
      "ì¡°íƒœì—´ ì¥ê´€, ì œ6ì°¨ í•œ-í˜¸ì£¼ ì™¸êµï½¥êµ­ë°©(2+2) ì¥ê´€íšŒì˜ ê°œìµœ");
  ASSERT_EQ(tokens.size(), 11);
  ASSERT_EQ(tokens[2], "ì œ6ì°¨");
  ASSERT_EQ(tokens[10], "ê°œìµœ");
  tokens = tokenizer->split(
      "Zu unseren Werten stehen, Risiken vorbeugen, aber auch Gemeinsamkeiten "
      "erkennen und Wege der Zusammenarbeit finden â€“ so sehe ich meine "
      "Aufgabe.  â€“ Botschafterin Dr. Patricia Flor");
  ASSERT_EQ(tokens.size(), 24);
  ASSERT_EQ(tokens[0], "zu");
  ASSERT_EQ(tokens[8], "gemeinsamkeiten");
  ASSERT_EQ(tokens[15], "so");
  ASSERT_EQ(tokens[21], "dr");
  ASSERT_EQ(tokens[23], "flor");
  tokens = tokenizer->split("5æœˆ7æ—¥ã€é‡‘æ‰å¤§ä½¿ã¯ã€åŒ—äº¬å¸‚åœ¨ä½ã®ä¸­æ‘äº¬å­ã•ã‚“ï¼ˆ93æ­³"
                            "ï¼‰ã‚’å…¬é‚¸ã«æ‹›ãã€é¢ä¼šã—ã¾ã—ãŸã€‚");
  ASSERT_EQ(tokens.size(), 31);
  ASSERT_EQ(tokens[0], "5");
  ASSERT_EQ(tokens[1], "æœˆ");
  ASSERT_EQ(tokens[19], "ã•ã‚“");
  ASSERT_EQ(tokens[23], "å…¬");
  ASSERT_EQ(tokens[30], "ã—ã¾ã—ãŸ");
  tokens = tokenizer->split(
      "Ø§Ø³ØªÙ‚Ø¨Ù„ Ù…Ø¹Ø§Ù„ÙŠ Ù†Ø§Ø¦Ø¨ ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ ÙˆÙ„ÙŠØ¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„ÙƒØ±ÙŠÙ… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠØŒ "
      "Ø§Ù„ÙŠÙˆÙ…ØŒ Ø¯ÙˆÙ„Ø© Ø±Ø¦ÙŠØ³ ÙˆØ²Ø±Ø§Ø¡ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ø¯ÙƒØªÙˆØ± ...");
  ASSERT_EQ(tokens.size(), 18);
  ASSERT_EQ(tokens[0], "Ø§Ø³ØªÙ‚Ø¨Ù„");
  ASSERT_EQ(tokens[17], "Ø§Ù„Ø¯ÙƒØªÙˆØ±");
}

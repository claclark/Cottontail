#include "src/utf8_tokenizer.h"

#include <memory>
#include <string>
#include <thread>
#include <vector>

#include "gtest/gtest.h"

#include "src/cottontail.h"

namespace {
const char *libai[] = {
    "è‡ªé£ (æç™½)  Entertaining myself (Li Bai)",
    "",
    "å¯¹é…’ä¸è§‰ç‘   Too drunk to notice it had gotten dark,",
    "è½èŠ±ç›ˆæˆ‘è¡£ã€‚ Or that flowers had fallen all over my clothes,",
    "é†‰èµ·æ­¥æºªæœˆï¼Œ I stagger up and walk along the moonlit stream,",
    "é¸Ÿè¿˜äººäº¦ç¨€ã€‚ The birds have gone home; most of the people too."};

void tokenize_test(std::shared_ptr<cottontail::Tokenizer> tokenizer,
                   std::shared_ptr<cottontail::Featurizer> featurizer,
                   const std::string &target) {
  size_t n = target.length() + 1;
  std::unique_ptr<char[]> buffer = std::unique_ptr<char[]>(new char[n]);
  strcpy(buffer.get(), target.c_str());
  std::vector<cottontail::Token> tokens =
      tokenizer->tokenize(featurizer, buffer.get(), n);
  std::vector<std::string> splits = tokenizer->split(target);
  EXPECT_EQ(tokens.size(), splits.size());
  for (size_t i = 0; i < tokens.size(); i++) {
    cottontail::addr feature = featurizer->featurize(splits[i]);
    EXPECT_EQ(tokens[i].feature, feature);
    EXPECT_EQ(tokens[i].address, (cottontail::addr)i);
    if (i > 0) {
      EXPECT_GT(tokens[i].offset, tokens[i - 1].offset);
    }
    EXPECT_NE(tokens[i].length, 0);
    std::vector<std::string> single =
        tokenizer->split(target.substr(tokens[i].offset, tokens[i].length));
    EXPECT_EQ(single.size(), 1);
    EXPECT_EQ(tokens[i].feature, featurizer->featurize(single[0]));
  }
}
} // namespace

TEST(Utf8Tokenizer, Tokenize) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer;
  tokenizer = cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  tokenizer = cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::shared_ptr<cottontail::Featurizer> featurizer =
      cottontail::Featurizer::make("hashing", "");
  tokenize_test(tokenizer, featurizer, "hello world");
  tokenize_test(tokenizer, featurizer, "HeLlO***WoRld");
  tokenize_test(tokenizer, featurizer, "");
  tokenize_test(tokenizer, featurizer, "!(*)*!*#&#&#)@");
  std::string s;
  size_t n = sizeof(libai) / sizeof(char *);
  for (size_t i = 0; i < n; i++)
    s += libai[i] + std::string("\n");
  tokenize_test(tokenizer, featurizer, s);
  tokenize_test(tokenizer, featurizer, "Mixingè‹±è¯­andä¸­æ–‡with no spaces");
  tokenize_test(
      tokenizer, featurizer,
      "ç¿’è¿‘å¹³å›½å®¶ä¸»å¸­ã®ãƒ‘ãƒªåˆ°ç€ã‚’å„ç•Œã®é–¢ä¿‚è€…ãŒæ²¿é“ã§æ­“è¿ï¼ˆ2024-05-08ï¼‰");
  tokenize_test(tokenizer, featurizer,
                "ì‹ ì‹œëŒ€ ì¤‘êµ­ì„ ì´í•´í•˜ê³  ë” ë‚˜ì€ ë¯¸ë˜ë¥¼ í•¨ê»˜ ë§Œë“¤ì–´ê°€ì - "
                "ì‹±í•˜ì´ë° ëŒ€ì‚¬ í•œêµ­ ìš°ì†¡ëŒ€í•™êµ ê°•ì—°ï¼ˆ2024-05-05ï¼‰");
  tokenize_test(
      tokenizer, featurizer,
      "FAIRE RAYONNER Lâ€™ESPRIT PRÃ‰SIDANT Ã€ Lâ€™Ã‰TABLISSEMENT DES RELATIONS "
      "DIPLOMATIQUES ENTRE LA CHINE ET LA FRANCE ET PROMOUVOIR ENSEMBLE LA "
      "PAIX ET LE DÃ‰VELOPPEMENT DANS LE MONDE");
  tokenize_test(
      tokenizer, featurizer,
      "Ğ’â€‚ÑÑ€ĞµĞ´Ñƒâ€‚Ğ²â€‚Ğ¢ÑƒĞ½ÑŒÑĞ¸â€‚/Ğ¿Ñ€Ğ¾Ğ².â€‚ĞĞ½ÑŒÑ…Ğ¾Ğ¹,â€‚Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹â€‚ĞšĞ¸Ñ‚Ğ°Ğ¹/"
      "â€‚Ñ‡Ğ»ĞµĞ½â€‚Ğ“Ğ¾ÑÑĞ¾Ğ²ĞµÑ‚Ğ°â€‚Ğ¸â€‚Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€â€‚Ğ¸Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ…â€‚Ğ´ĞµĞ»â€‚ĞšĞĞ â€‚Ğ’Ğ°Ğ½â€‚Ğ˜â€‚Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»â€‚Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹â€‚Ñâ€‚"
      "Ğ³Ğ»Ğ°Ğ²Ğ¾Ğ¹â€‚ĞœĞ˜Ğ”â€‚Ğ Ğ¤â€‚Ğ¡ĞµÑ€Ğ³ĞµĞµĞ¼â€‚Ğ›Ğ°Ğ²Ñ€Ğ¾Ğ²Ñ‹Ğ¼,"
      "â€‚Ğ²â€‚Ñ…Ğ¾Ğ´Ğµâ€‚ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ…â€‚Ğ¾Ğ±Ğµâ€‚ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹â€‚Ğ¿Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°Ğ»Ğ¸â€‚ÑƒĞºÑ€ĞµĞ¿Ğ»ÑÑ‚ÑŒâ€‚Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğµâ€‚ÑĞ²ÑĞ·Ğ¸");
  tokenize_test(tokenizer, featurizer,
                "ÙÙŠ ÙŠÙˆÙ… 19 ÙŠÙˆÙ„ÙŠÙˆ Ø¹Ø§Ù… 2021 Ø¨Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…Ø­Ù„ÙŠØŒ Ø¹Ù‚Ø¯ Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¯ÙˆÙ„Ø© "
                "ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„ØµÙŠÙ†ÙŠ ÙˆØ§Ù†Øº ÙŠÙŠ Ù…Ø¤ØªÙ…Ø±Ø§ ØµØ­ÙÙŠØ§ Ù…Ø´ØªØ±ÙƒØ§ Ù…Ø¹ ÙˆØ²ÙŠØ± "
                "Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø±Ù…Ø·Ø§Ù† Ù„Ø¹Ù…Ø§Ù…Ø±Ø© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±.");

  // Test with some characters whose UTF-8 byte-length changes when case folded.
  // Unicode Character â€œÅ¿â€ (U+017F) Latin Small Letter Long S
  // Unicode Character "Èº" (U+023A) Latin Capital Letter A With Stroke
  // Unicode Character "â„ª" (U+212A) - Kelvin sign
  s = "Å¿Å¿Å¿ aaaÅ¿bbbÅ¿ Å¿Å¿Å¿Å¿Å¿Å¿ 1234 Å¿";
  tokenize_test(tokenizer, featurizer, s);
  s = "ÈºBC ÈºÈºÈº Èº1Èº ÈºÈºÈºÈº aAÈºAa";
  tokenize_test(tokenizer, featurizer, s);
  s = "100â„ª is Å¿Èºâ„ªing cold";
  tokenize_test(tokenizer, featurizer, s);
}

TEST(Utf8Tokenizer, Split) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer =
      cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::vector<std::string> tokens;
  tokens = tokenizer->split("hello world");
  EXPECT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0], "hello");
  EXPECT_EQ(tokens[1], "world");
  tokens = tokenizer->split("HeLlO     wOrLd");
  EXPECT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0], "hello");
  EXPECT_EQ(tokens[1], "world");
  tokens = tokenizer->split("");
  EXPECT_EQ(tokens.size(), 0);
  tokens = tokenizer->split("ã€‚. ...");
  EXPECT_EQ(tokens.size(), 0);
  std::string s;
  size_t n = sizeof(libai) / sizeof(char *);
  for (size_t i = 0; i < n; i++)
    s += libai[i] + std::string("\n");
  tokens = tokenizer->split(s);
  EXPECT_EQ(tokens.size(), 64);
  EXPECT_EQ(tokens[0], "è‡ª");
  EXPECT_EQ(tokens[2], "æ");
  EXPECT_EQ(tokens[6], "li");
  EXPECT_EQ(tokens[7], "bai");
  EXPECT_EQ(tokens[24], "æˆ‘");
  EXPECT_EQ(tokens[34], "clothes");
  EXPECT_EQ(tokens[40], "i");
  EXPECT_EQ(tokens[53], "ç¨€");
  EXPECT_EQ(tokens[54], "the");
  EXPECT_EQ(tokens[62], "people");
  EXPECT_EQ(tokens[63], "too");
  tokens = tokenizer->split(
      "ÎœÎ®Î½Ï…Î¼Î± Ï„Î·Ï‚ Î ÏÎ¿Î­Î´ÏÎ¿Ï… Ï„Î·Ï‚ Î”Î·Î¼Î¿ÎºÏÎ±Ï„Î¯Î±Ï‚ ÎšÎ±Ï„ÎµÏÎ¯Î½Î±Ï‚ Î£Î±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï… Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ "
      "Î±Ï€ÏŒÎ´Î·Î¼Î¿ Î•Î»Î»Î·Î½Î¹ÏƒÎ¼ÏŒ Î¼Îµ Ï„Î·Î½ ÎµÏ…ÎºÎ±Î¹ÏÎ¯Î± Ï„Î·Ï‚ ÎµÎ¸Î½Î¹ÎºÎ®Ï‚ ÎµÎ¿ÏÏ„Î®Ï‚ Ï„Î·Ï‚ 25Î·Ï‚ ÎœÎ±ÏÏ„Î¯Î¿Ï….");
  EXPECT_EQ(tokens.size(), 20);
  EXPECT_EQ(tokens[0], "Î¼Î®Î½Ï…Î¼Î±");
  EXPECT_EQ(tokens[6], "ÏƒÎ±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï…");
  EXPECT_EQ(tokens[18], "25Î·Ïƒ");
  EXPECT_EQ(tokens[19], "Î¼Î±ÏÏ„Î¯Î¿Ï…");
  tokens =
      tokenizer->split("ğŸ’™Blue HeartğŸ’—Growing Heart ğŸ’šç»¿è‰²çˆ±å¿ƒğŸ’– Sparkling "
                       "Heart ğŸ’“ Beating Heart ğŸ–¤ Black Heart ğŸ’Ÿ Heart "
                       "Decoration ğŸ’” Broken Heart ğŸ’› Yellow Heart");

  EXPECT_EQ(tokens.size(), 29);
  EXPECT_EQ(tokens[0], "ğŸ’™");
  EXPECT_EQ(tokens[5], "heart");
  EXPECT_EQ(tokens[10], "å¿ƒ");
  EXPECT_EQ(tokens[14], "ğŸ’“");
  EXPECT_EQ(tokens[28], "heart");
  tokens = tokenizer->split(
      "ì¡°íƒœì—´ ì¥ê´€, ì œ6ì°¨ í•œ-í˜¸ì£¼ ì™¸êµï½¥êµ­ë°©(2+2) ì¥ê´€íšŒì˜ ê°œìµœ");
  EXPECT_EQ(tokens.size(), 11);
  EXPECT_EQ(tokens[2], "ì œ6ì°¨");
  EXPECT_EQ(tokens[10], "ê°œìµœ");
  tokens = tokenizer->split(
      "Zu unseren Werten stehen, Risiken vorbeugen, aber auch Gemeinsamkeiten "
      "erkennen und Wege der Zusammenarbeit finden â€“ so sehe ich meine "
      "Aufgabe.  â€“ Botschafterin Dr. Patricia Flor");
  EXPECT_EQ(tokens.size(), 24);
  EXPECT_EQ(tokens[0], "zu");
  EXPECT_EQ(tokens[8], "gemeinsamkeiten");
  EXPECT_EQ(tokens[15], "so");
  EXPECT_EQ(tokens[21], "dr");
  EXPECT_EQ(tokens[23], "flor");
  tokens = tokenizer->split("5æœˆ7æ—¥ã€é‡‘æ‰å¤§ä½¿ã¯ã€åŒ—äº¬å¸‚åœ¨ä½ã®ä¸­æ‘äº¬å­ã•ã‚“ï¼ˆ93æ­³"
                            "ï¼‰ã‚’å…¬é‚¸ã«æ‹›ãã€é¢ä¼šã—ã¾ã—ãŸã€‚");
  EXPECT_EQ(tokens.size(), 31);
  EXPECT_EQ(tokens[0], "5");
  EXPECT_EQ(tokens[1], "æœˆ");
  EXPECT_EQ(tokens[19], "ã•ã‚“");
  EXPECT_EQ(tokens[23], "å…¬");
  EXPECT_EQ(tokens[30], "ã—ã¾ã—ãŸ");
  tokens = tokenizer->split(
      "Ø§Ø³ØªÙ‚Ø¨Ù„ Ù…Ø¹Ø§Ù„ÙŠ Ù†Ø§Ø¦Ø¨ ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ ÙˆÙ„ÙŠØ¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„ÙƒØ±ÙŠÙ… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠØŒ "
      "Ø§Ù„ÙŠÙˆÙ…ØŒ Ø¯ÙˆÙ„Ø© Ø±Ø¦ÙŠØ³ ÙˆØ²Ø±Ø§Ø¡ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ø¯ÙƒØªÙˆØ± ...");
  EXPECT_EQ(tokens.size(), 18);
  EXPECT_EQ(tokens[0], "Ø§Ø³ØªÙ‚Ø¨Ù„");
  EXPECT_EQ(tokens[17], "Ø§Ù„Ø¯ÙƒØªÙˆØ±");
}

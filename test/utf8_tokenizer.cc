#include "src/utf8_tokenizer.h"

#include <memory>
#include <string>
#include <thread>
#include <vector>

#include "gtest/gtest.h"

#include "src/cottontail.h"

namespace {
const char *libai[] = {
    "è‡ªé£ (æç™½)  Entertaining myself (Li Bai)",
    "",
    "å¯¹é…’ä¸è§‰ç‘   Too drunk to notice it had gotten dark,",
    "è½èŠ±ç›ˆæˆ‘è¡£ã€‚ Or that flowers had fallen all over my clothes,",
    "é†‰èµ·æ­¥æºªæœˆï¼Œ I stagger up and walk along the moonlit stream,",
    "é¸Ÿè¿˜äººäº¦ç¨€ã€‚ The birds have gone home; most of the people too."};

void tokenize_test(std::shared_ptr<cottontail::Tokenizer> tokenizer,
                   std::shared_ptr<cottontail::Featurizer> featurizer,
                   const std::string &target) {
  size_t n = target.length() + 1;
  std::unique_ptr<char[]> buffer = std::unique_ptr<char[]>(new char[n]);
  strcpy(buffer.get(), target.c_str());
  std::vector<cottontail::Token> tokens =
      tokenizer->tokenize(featurizer, buffer.get(), n);
  std::vector<std::string> splits = tokenizer->split(target);
  EXPECT_EQ(tokens.size(), splits.size());
  for (size_t i = 0; i < tokens.size(); i++) {
    cottontail::addr feature = featurizer->featurize(splits[i]);
    EXPECT_EQ(tokens[i].feature, feature);
    EXPECT_EQ(tokens[i].address, (cottontail::addr)i);
    if (i > 0) {
      EXPECT_GT(tokens[i].offset, tokens[i - 1].offset);
    }
    EXPECT_NE(tokens[i].length, 0);
    std::vector<std::string> single =
        tokenizer->split(target.substr(tokens[i].offset, tokens[i].length));
    EXPECT_EQ(single.size(), 1);
    EXPECT_EQ(tokens[i].feature, featurizer->featurize(single[0]));
  }
}
} // namespace

TEST(Utf8Tokenizer, Tokenize) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer;
  tokenizer = cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  tokenizer = cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::shared_ptr<cottontail::Featurizer> featurizer =
      cottontail::Featurizer::make("hashing", "");
  ASSERT_NE(featurizer, nullptr);
  tokenize_test(tokenizer, featurizer, "hello world");
  tokenize_test(tokenizer, featurizer, "HeLlO***WoRld");
  tokenize_test(tokenizer, featurizer, "");
  tokenize_test(tokenizer, featurizer, "!(*)*!*#&#&#)@");
  std::string s;
  size_t n = sizeof(libai) / sizeof(char *);
  for (size_t i = 0; i < n; i++)
    s += libai[i] + std::string("\n");
  tokenize_test(tokenizer, featurizer, s);
  tokenize_test(tokenizer, featurizer, "Mixingè‹±è¯­andä¸­æ–‡with no spaces");
  tokenize_test(
      tokenizer, featurizer,
      "ç¿’è¿‘å¹³å›½å®¶ä¸»å¸­ã®ãƒ‘ãƒªåˆ°ç€ã‚’å„ç•Œã®é–¢ä¿‚è€…ãŒæ²¿é“ã§æ­“è¿ï¼ˆ2024-05-08ï¼‰");
  tokenize_test(tokenizer, featurizer,
                "ì‹ ì‹œëŒ€ ì¤‘êµ­ì„ ì´í•´í•˜ê³  ë” ë‚˜ì€ ë¯¸ë˜ë¥¼ í•¨ê»˜ ë§Œë“¤ì–´ê°€ì - "
                "ì‹±í•˜ì´ë° ëŒ€ì‚¬ í•œêµ­ ìš°ì†¡ëŒ€í•™êµ ê°•ì—°ï¼ˆ2024-05-05ï¼‰");
  tokenize_test(
      tokenizer, featurizer,
      "FAIRE RAYONNER Lâ€™ESPRIT PRÃ‰SIDANT Ã€ Lâ€™Ã‰TABLISSEMENT DES RELATIONS "
      "DIPLOMATIQUES ENTRE LA CHINE ET LA FRANCE ET PROMOUVOIR ENSEMBLE LA "
      "PAIX ET LE DÃ‰VELOPPEMENT DANS LE MONDE");
  tokenize_test(
      tokenizer, featurizer,
      "Ğ’â€‚ÑÑ€ĞµĞ´Ñƒâ€‚Ğ²â€‚Ğ¢ÑƒĞ½ÑŒÑĞ¸â€‚/Ğ¿Ñ€Ğ¾Ğ².â€‚ĞĞ½ÑŒÑ…Ğ¾Ğ¹,â€‚Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹â€‚ĞšĞ¸Ñ‚Ğ°Ğ¹/"
      "â€‚Ñ‡Ğ»ĞµĞ½â€‚Ğ“Ğ¾ÑÑĞ¾Ğ²ĞµÑ‚Ğ°â€‚Ğ¸â€‚Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€â€‚Ğ¸Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ…â€‚Ğ´ĞµĞ»â€‚ĞšĞĞ â€‚Ğ’Ğ°Ğ½â€‚Ğ˜â€‚Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»â€‚Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹â€‚Ñâ€‚"
      "Ğ³Ğ»Ğ°Ğ²Ğ¾Ğ¹â€‚ĞœĞ˜Ğ”â€‚Ğ Ğ¤â€‚Ğ¡ĞµÑ€Ğ³ĞµĞµĞ¼â€‚Ğ›Ğ°Ğ²Ñ€Ğ¾Ğ²Ñ‹Ğ¼,"
      "â€‚Ğ²â€‚Ñ…Ğ¾Ğ´Ğµâ€‚ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ…â€‚Ğ¾Ğ±Ğµâ€‚ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹â€‚Ğ¿Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°Ğ»Ğ¸â€‚ÑƒĞºÑ€ĞµĞ¿Ğ»ÑÑ‚ÑŒâ€‚Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğµâ€‚ÑĞ²ÑĞ·Ğ¸");
  tokenize_test(tokenizer, featurizer,
                "ÙÙŠ ÙŠÙˆÙ… 19 ÙŠÙˆÙ„ÙŠÙˆ Ø¹Ø§Ù… 2021 Ø¨Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…Ø­Ù„ÙŠØŒ Ø¹Ù‚Ø¯ Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¯ÙˆÙ„Ø© "
                "ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„ØµÙŠÙ†ÙŠ ÙˆØ§Ù†Øº ÙŠÙŠ Ù…Ø¤ØªÙ…Ø±Ø§ ØµØ­ÙÙŠØ§ Ù…Ø´ØªØ±ÙƒØ§ Ù…Ø¹ ÙˆØ²ÙŠØ± "
                "Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø±Ù…Ø·Ø§Ù† Ù„Ø¹Ù…Ø§Ù…Ø±Ø© ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±.");

  // Test with some characters whose UTF-8 byte-length changes when case folded.
  // Unicode Character â€œÅ¿â€ (U+017F) Latin Small Letter Long S
  // Unicode Character "Èº" (U+023A) Latin Capital Letter A With Stroke
  // Unicode Character "â„ª" (U+212A) - Kelvin sign
  s = "Å¿Å¿Å¿ aaaÅ¿bbbÅ¿ Å¿Å¿Å¿Å¿Å¿Å¿ 1234 Å¿";
  tokenize_test(tokenizer, featurizer, s);
  s = "ÈºBC ÈºÈºÈº Èº1Èº ÈºÈºÈºÈº aAÈºAa";
  tokenize_test(tokenizer, featurizer, s);
  s = "100â„ª is Å¿Èºâ„ªing cold";
  tokenize_test(tokenizer, featurizer, s);
}

TEST(Utf8Tokenizer, Split) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer =
      cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::vector<std::string> tokens;
  tokens = tokenizer->split("hello world");
  EXPECT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0], "hello");
  EXPECT_EQ(tokens[1], "world");
  tokens = tokenizer->split("HeLlO     wOrLd");
  EXPECT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0], "hello");
  EXPECT_EQ(tokens[1], "world");
  tokens = tokenizer->split("");
  EXPECT_EQ(tokens.size(), 0);
  tokens = tokenizer->split("ã€‚. ...");
  EXPECT_EQ(tokens.size(), 0);
  std::string s;
  size_t n = sizeof(libai) / sizeof(char *);
  for (size_t i = 0; i < n; i++)
    s += libai[i] + std::string("\n");
  tokens = tokenizer->split(s);
  EXPECT_EQ(tokens.size(), 64);
  EXPECT_EQ(tokens[0], "è‡ª");
  EXPECT_EQ(tokens[2], "æ");
  EXPECT_EQ(tokens[6], "li");
  EXPECT_EQ(tokens[7], "bai");
  EXPECT_EQ(tokens[24], "æˆ‘");
  EXPECT_EQ(tokens[34], "clothes");
  EXPECT_EQ(tokens[40], "i");
  EXPECT_EQ(tokens[53], "ç¨€");
  EXPECT_EQ(tokens[54], "the");
  EXPECT_EQ(tokens[62], "people");
  EXPECT_EQ(tokens[63], "too");
  tokens = tokenizer->split(
      "ÎœÎ®Î½Ï…Î¼Î± Ï„Î·Ï‚ Î ÏÎ¿Î­Î´ÏÎ¿Ï… Ï„Î·Ï‚ Î”Î·Î¼Î¿ÎºÏÎ±Ï„Î¯Î±Ï‚ ÎšÎ±Ï„ÎµÏÎ¯Î½Î±Ï‚ Î£Î±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï… Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ "
      "Î±Ï€ÏŒÎ´Î·Î¼Î¿ Î•Î»Î»Î·Î½Î¹ÏƒÎ¼ÏŒ Î¼Îµ Ï„Î·Î½ ÎµÏ…ÎºÎ±Î¹ÏÎ¯Î± Ï„Î·Ï‚ ÎµÎ¸Î½Î¹ÎºÎ®Ï‚ ÎµÎ¿ÏÏ„Î®Ï‚ Ï„Î·Ï‚ 25Î·Ï‚ ÎœÎ±ÏÏ„Î¯Î¿Ï….");
  EXPECT_EQ(tokens.size(), 20);
  EXPECT_EQ(tokens[0], "Î¼Î®Î½Ï…Î¼Î±");
  EXPECT_EQ(tokens[6], "ÏƒÎ±ÎºÎµÎ»Î»Î±ÏÎ¿Ï€Î¿ÏÎ»Î¿Ï…");
  EXPECT_EQ(tokens[18], "25Î·Ïƒ");
  EXPECT_EQ(tokens[19], "Î¼Î±ÏÏ„Î¯Î¿Ï…");
  tokens =
      tokenizer->split("ğŸ’™Blue HeartğŸ’—Growing Heart ğŸ’šç»¿è‰²çˆ±å¿ƒğŸ’– Sparkling "
                       "Heart ğŸ’“ Beating Heart ğŸ–¤ Black Heart ğŸ’Ÿ Heart "
                       "Decoration ğŸ’” Broken Heart ğŸ’› Yellow Heart");

  EXPECT_EQ(tokens.size(), 29);
  EXPECT_EQ(tokens[0], "ğŸ’™");
  EXPECT_EQ(tokens[5], "heart");
  EXPECT_EQ(tokens[10], "å¿ƒ");
  EXPECT_EQ(tokens[14], "ğŸ’“");
  EXPECT_EQ(tokens[28], "heart");
  tokens = tokenizer->split(
      "ì¡°íƒœì—´ ì¥ê´€, ì œ6ì°¨ í•œ-í˜¸ì£¼ ì™¸êµï½¥êµ­ë°©(2+2) ì¥ê´€íšŒì˜ ê°œìµœ");
  EXPECT_EQ(tokens.size(), 11);
  EXPECT_EQ(tokens[2], "ì œ6ì°¨");
  EXPECT_EQ(tokens[10], "ê°œìµœ");
  tokens = tokenizer->split(
      "Zu unseren Werten stehen, Risiken vorbeugen, aber auch Gemeinsamkeiten "
      "erkennen und Wege der Zusammenarbeit finden â€“ so sehe ich meine "
      "Aufgabe.  â€“ Botschafterin Dr. Patricia Flor");
  EXPECT_EQ(tokens.size(), 24);
  EXPECT_EQ(tokens[0], "zu");
  EXPECT_EQ(tokens[8], "gemeinsamkeiten");
  EXPECT_EQ(tokens[15], "so");
  EXPECT_EQ(tokens[21], "dr");
  EXPECT_EQ(tokens[23], "flor");
  tokens = tokenizer->split("5æœˆ7æ—¥ã€é‡‘æ‰å¤§ä½¿ã¯ã€åŒ—äº¬å¸‚åœ¨ä½ã®ä¸­æ‘äº¬å­ã•ã‚“ï¼ˆ93æ­³"
                            "ï¼‰ã‚’å…¬é‚¸ã«æ‹›ãã€é¢ä¼šã—ã¾ã—ãŸã€‚");
  EXPECT_EQ(tokens.size(), 31);
  EXPECT_EQ(tokens[0], "5");
  EXPECT_EQ(tokens[1], "æœˆ");
  EXPECT_EQ(tokens[19], "ã•ã‚“");
  EXPECT_EQ(tokens[23], "å…¬");
  EXPECT_EQ(tokens[30], "ã—ã¾ã—ãŸ");
  tokens = tokenizer->split(
      "Ø§Ø³ØªÙ‚Ø¨Ù„ Ù…Ø¹Ø§Ù„ÙŠ Ù†Ø§Ø¦Ø¨ ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ ÙˆÙ„ÙŠØ¯ Ø¨Ù† Ø¹Ø¨Ø¯Ø§Ù„ÙƒØ±ÙŠÙ… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠØŒ "
      "Ø§Ù„ÙŠÙˆÙ…ØŒ Ø¯ÙˆÙ„Ø© Ø±Ø¦ÙŠØ³ ÙˆØ²Ø±Ø§Ø¡ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ø¯ÙƒØªÙˆØ± ...");
  EXPECT_EQ(tokens.size(), 18);
  EXPECT_EQ(tokens[0], "Ø§Ø³ØªÙ‚Ø¨Ù„");
  EXPECT_EQ(tokens[17], "Ø§Ù„Ø¯ÙƒØªÙˆØ±");
}

namespace {
void skip_test(std::shared_ptr<cottontail::Tokenizer> tokenizer,
               std::shared_ptr<cottontail::Featurizer> featurizer,
               const std::string &target) {
  size_t n = target.length() + 1;
  std::unique_ptr<char[]> buffer = std::unique_ptr<char[]>(new char[n]);
  strcpy(buffer.get(), target.c_str());
  std::vector<cottontail::Token> tokens =
      tokenizer->tokenize(featurizer, buffer.get(), n);
  for (size_t i = 0; i < tokens.size(); i++) {
    const char *remainder = tokenizer->skip(target.c_str(), target.length(), i);
    EXPECT_EQ(target.length(), tokens[i].offset + strlen(remainder));
  }
}
} // namespace

TEST(Utf8Tokenizer, Skip) {
  std::shared_ptr<cottontail::Tokenizer> tokenizer =
      cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::shared_ptr<cottontail::Featurizer> featurizer =
      cottontail::Featurizer::make("hashing", "");
  ASSERT_NE(featurizer, nullptr);
  std::string s;
  s = "(((((hello ((world)))))))";
  skip_test(tokenizer, featurizer, std::string(s));
  EXPECT_STREQ("hello ((world)))))))",
               tokenizer->skip(s.c_str(), s.length(), 0));
  EXPECT_STREQ("world)))))))", tokenizer->skip(s.c_str(), s.length(), 1));
  EXPECT_STREQ("", tokenizer->skip(s.c_str(), s.length(), 2));
  EXPECT_STREQ("", tokenizer->skip(s.c_str(), s.length(), 3));
  s = libai[0];
  skip_test(tokenizer, featurizer, std::string(s));
  EXPECT_STREQ("è‡ªé£ (æç™½)  Entertaining myself (Li Bai)",
               tokenizer->skip(s.c_str(), s.length(), 0));
  EXPECT_STREQ("ç™½)  Entertaining myself (Li Bai)",
               tokenizer->skip(s.c_str(), s.length(), 3));
  EXPECT_STREQ("Bai)", tokenizer->skip(s.c_str(), s.length(), 7));
  EXPECT_STREQ("", tokenizer->skip(s.c_str(), s.length(), 999));
  skip_test(tokenizer, featurizer, std::string(s));
  s = "Mixingè‹±è¯­andä¸­æ–‡with no spaces";
  skip_test(tokenizer, featurizer, std::string(s));
  EXPECT_STREQ("Mixingè‹±è¯­andä¸­æ–‡with no spaces",
               tokenizer->skip(s.c_str(), s.length(), -99));
  EXPECT_STREQ("Mixingè‹±è¯­andä¸­æ–‡with no spaces",
               tokenizer->skip(s.c_str(), s.length(), 0));
  EXPECT_STREQ("è¯­andä¸­æ–‡with no spaces",
               tokenizer->skip(s.c_str(), s.length(), 2));
  EXPECT_STREQ("andä¸­æ–‡with no spaces",
               tokenizer->skip(s.c_str(), s.length(), 3));
  EXPECT_STREQ("ä¸­æ–‡with no spaces", tokenizer->skip(s.c_str(), s.length(), 4));
  EXPECT_STREQ("æ–‡with no spaces", tokenizer->skip(s.c_str(), s.length(), 5));
  EXPECT_STREQ("with no spaces", tokenizer->skip(s.c_str(), s.length(), 6));
  EXPECT_STREQ("no spaces", tokenizer->skip(s.c_str(), s.length(), 7));
  EXPECT_STREQ("spaces", tokenizer->skip(s.c_str(), s.length(), 8));
  EXPECT_STREQ("", tokenizer->skip(s.c_str(), s.length(), 9));
  EXPECT_STREQ("", tokenizer->skip(s.c_str(), s.length(), 9999));
  s = "...If you need to find an ATM in order to withdraw ğŸ’² money, then send "
      "the combination ğŸ†˜ğŸ§ â€” â€œWhere is the ATM?â€ by combining just two "
      "emoticons â€“ emoji ğŸ†˜ SOS sign and ğŸ§ ATM sign emojis.";
  skip_test(tokenizer, featurizer, std::string(s));
  s = "ï¼”æœˆ26æ—¥ã€æ¥Šå®‡é§æ—¥è‡¨æ™‚ä»£ç†å¤§ä½¿ã¯æ—¥æœ¬å‰µä¾¡å­¦ä¼šã®æ‹›ãã«å¿œã˜ã€é–“ã‚‚ãªãä¸­å›½ã‚’"
      "è¨ªã‚Œã‚‹äºˆå®šã®å‰µä¾¡å­¦ä¼šã®ä»£è¡¨å›£ã«å‘ã‘ã¦ã‚¹ãƒ”ãƒ¼ãƒã—ãŸã€‚";
  skip_test(tokenizer, featurizer, std::string(s));
  s = "Capi was dormido aquÃ­.  The girl say, â€œÂ¿QuÃ© podemos hacer?â€ to the boy. "
      " Hacen una casa.  He was making the house and he cut out a door.  Y Ã©l "
      "tiene su cobija y estÃ¡ happy.";
  skip_test(tokenizer, featurizer, std::string(s));
  s = "skip_test(tokenizer, featurizer, std::string(s))";
  skip_test(tokenizer, featurizer, std::string(s));
  s = ">>>>CÃ¡ch Ä‘Ã¢y 70 nÄƒm, Hiá»‡p Ä‘á»‹nh Geneva vá» Ä‘Ã¬nh chá»‰ chiáº¿n sá»± á»Ÿ Viá»‡t Nam "
      "Ä‘Ã£ Ä‘Æ°á»£c kÃ½ káº¿t, má»Ÿ ra má»™t trang má»›i trong sá»± nghiá»‡p Ä‘áº¥u tranh giáº£i "
      "phÃ³ng dÃ¢n tá»™c, thá»‘ng nháº¥t Ä‘áº¥t nÆ°á»›c cá»§a nhÃ¢n dÃ¢n ta. Tráº£i qua 70 nÄƒm, "
      "nhá»¯ng bÃ i há»c tá»« Ä‘Ã m phÃ¡n, kÃ½ káº¿t vÃ  thá»±c thi Hiá»‡p Ä‘á»‹nh Geneva váº«n cÃ²n "
      "nguyÃªn giÃ¡ trá»‹ Ä‘á»‘i vá»›i cÃ´ng cuá»™c xÃ¢y dá»±ng, phÃ¡t triá»ƒn Ä‘áº¥t nÆ°á»›c vÃ  báº£o "
      "vá»‡ Tá»• quá»‘c ngÃ y nay.<<<<<";
  skip_test(tokenizer, featurizer, std::string(s));
  s = "ã€‚ã€‚ã€‚ã€‚ã€‚à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™à¸à¹ˆà¸²à¸¢à¸à¸‡à¸ªà¸¸à¸¥â€‚â€‚à¸ªà¸–à¸²à¸™à¸—à¸¹à¸•à¸ˆà¸µà¸™à¸ˆà¸°à¸›à¸´à¸”à¸—à¸³à¸à¸²à¸£à¹ƒà¸™à¸§à¸±à¸™à¸—à¸µà¹ˆ 1 â€“ 5â€‚à¸à¸¤à¸©à¸ à¸²à¸„à¸¡ à¸.à¸¨.2567 "
      "à¹€à¸™à¸·à¹ˆà¸­à¸‡à¹ƒà¸™à¸§à¸±à¸™à¹à¸£à¸‡à¸‡à¸²à¸™ à¸§à¸±à¸™à¸—à¸µà¹ˆ 6 à¸à¸¤à¸©à¸ à¸²à¸„à¸¡ à¸.à¸¨.2567 à¹€à¸™à¸·à¹ˆà¸­à¸‡à¹ƒà¸™à¸§à¸±à¸™à¸«à¸¢à¸¸à¸”à¸Šà¸”à¹€à¸Šà¸¢à¸§à¸±à¸™à¸‰à¸±à¸•à¸£à¸¡à¸‡à¸„à¸¥ à¸§à¸±à¸™à¸—à¸µà¹ˆ 22 "
      "à¸à¸¤à¸©à¸ à¸²à¸„à¸¡ à¸.à¸¨.2567 à¹€à¸™à¸·à¹ˆà¸­à¸‡à¹ƒà¸™à¸§à¸±à¸™à¸§à¸´à¸ªà¸²à¸‚à¸šà¸¹à¸Šà¸²ã€‚ã€‚ã€‚ã€‚";
  skip_test(tokenizer, featurizer, std::string(s));
}

TEST(Utf8Tokenizer, Boundary) {
  std::shared_ptr<cottontail::Featurizer> featurizer =
      cottontail::Featurizer::make("hashing", "");
  ASSERT_NE(featurizer, nullptr);
  std::shared_ptr<cottontail::Tokenizer> tokenizer =
      cottontail::Tokenizer::make("utf8", "");
  ASSERT_NE(tokenizer, nullptr);
  std::string text = "0.5000";
  std::vector<cottontail::Token> tokens = tokenizer->tokenize(featurizer, text);
  EXPECT_EQ(tokens.size(), 2);
  EXPECT_EQ(tokens[0].address, 0);
  EXPECT_EQ(tokens[0].offset, 0);
  EXPECT_EQ(tokens[0].length, 1);
  EXPECT_EQ(tokens[1].address, 1);
  EXPECT_EQ(tokens[1].offset, 2);
  EXPECT_EQ(tokens[1].length, 4);
  std::vector<std::string> splits = tokenizer->split(text);
  EXPECT_EQ(splits.size(), 2);
  EXPECT_EQ(splits[0], "0");
  EXPECT_EQ(splits[1], "5000");
}
